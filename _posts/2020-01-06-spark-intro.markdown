---
title : "스파크 (1) : 하둡과 MapReduce"
excerpt : "하둡, MapReduce, HDFS, YARN"
category :
  - spark
tag :
  - spark
use_math : true
author_profile : true
header:
  teaser : /assets/images/category/data.jpg
  overlay_image : /assets/images/category/data.jpg
  overlay_filter: 0.1
---

## 하둡

![hadoop](../assets/img/spark/hadoop.png) 


아파치 하둡이란 빅데이터를 저장, 처리, 분석할 수 있는 소프트웨어 프레임워크입니다. 구글은 빅데이터를 모아서 처리하기 위한 프레임워크의 필요성을 느끼게 되어 자체적으로 빅데이터 처리 프레임워크를 제작하였습니다. 한편 아파치 재단은 전세계에 프레임워크를 무료로 보급해야 한다는 필요성을 느꼈고, 그 필요에 의해 구글의 방식을 따라서 오픈 소스 형태로 비슷한 프레임워크를 만들어서 제공하게 됩니다. 이것이 하둡 프로젝트입니다. 


## 기존의 분산 처리 시스템의 문제점

기존의 분산 처리 시스템은 Message Passing Interface를 사용하였습니다. 여기에는 몇가지 문제가 있었는데요, 일단 **프로그래밍이 매우 복잡**하였으며 여러 대의 컴퓨터 중 **일부 컴퓨터가 고장하는 경우 전체 시스템이 동작하지 않는** partial failure 문제가 있었습니다. 

## 하둡 : 새로운 ``파일시스템``과 ``처리시스템``을 도입하다

2000년대 초반에 구글은 연구를 통해 자체적인 file system과 mapreduce 방법을 생각해냈습니다. 

- 구글이 2003년에 ``구글 파일 시스템``이라는 논문을, 2004년에는 ``맵리듀스:대형 클러스터에서의 단순화된 데이터 처리``라는 논문을 발표하였습니다..
- 두 논문은 검색 엔진에서 방대한 양의 텍스트 데이터의 접근 방식에 대해 설명한 논문이라고 할 수 있습니다. 

이에, 아파치 재단은 전세계에 이러한 프레임워크를 **무료로** 보급해야 한다는 필요성을 느꼈고, 그 필요에 의해 구글의 방식을 따라서 오픈 소스 형태로 비슷한 프레임워크를 만들어서 제공하게 되었습니다. 이것이 바로 ``하둡``입니다. 하둡 분산 처리 시스템은 다음과 같은 특성이 있습니다.

1. 프로그래밍하기 비교적 쉽습니다.  
2. 각 노드는 가급적 최소한의 데이터를 주고 받습니다.   
3. 데이터는 여러 노드에 **미리 분산되어 저장**하며, **저장된 위치에서 연산**을 수행합니다.

설명하자면, 데이터는 블록으로 나누어져서, 파일 연산은 **블록 단위**로 이루어집니다. 하둡은 모든 연산을 ``mapreduce``방식으로 하는데요, 이 중에서 map task는 하나의 block단위로 작업이 처리됩니다. 

## 하둡의 구성요소 

<center><b><p></p>
 하둡 클러스터 = HDFS + YARN <p></p>
</b></center>

하둡은 파일 시스템인 ``HDFS``와 스케줄링 시스템인 ``YARN`` 이렇게 두가지로 구성되어 있습니다. HDFS 클러스터와 YARN 클러스터가 서로 결합된 조합을 ``하둡 클러스터``라고 합니다.

## HDFS : 하둡 분산 파일 시스템

HDFS (하둡 분산 파일 시스템, Hadoop Distributed File System)은 하둡의 스토리지 서브 시스템을 지칭합니다.

### **HDFS 파일 시스템의 구성요소** 

|구성요소|부연설명|역할|
|---|---|---|
|HDFS 클라이언트||name node를 통해 정보를 받아서 data 노드와 직접 통신을 합니다.|
|Name 노드|HDFS의 Master 노드|slave 노드에 대한 정보와 task 관리를 합니다.|
|Data 노드|HDFS의 Slave 노드|실제로 데이터를 보유하며, client 요청을 받아서 데이터를 전달하거나 task를 수행합니다.|


### **HDFS의 저장방식**
1. file은 block단위로 쪼개집니다. (ingestion 프로세스)
2. 같은 file의 여러개의 block은 서로 다른 여러개의 노드에 저장됩니다. 이때 block은 여러개로 복제(기본적으로 같은 block은 3개씩 복제)되어 서로 다른 노드에 저장됩니다. 
4. Meta data : Name 노드 (= Master 노드)는 file을 구성하는 block에 대한 정보와 그 block이 어디에 저장되어 있는지에 대한 정보를 관리합니다. 이러한 정보를 meta data라고 합니다. 
5. HDFS 클라이언트는 Name 노드로부터 블록 위치를 제공받습니다. 제공받은 meta data를 토대로 data 노드와 직접 통신합니다.

### **HDFS의 읽기방식**
1. HDFS 클라이언트는 Name 노드에게 파일 읽기를 요청합니다.
2. Name 노드는 meta data에 포함된 정보(요청된 파일의 블록이 포함된 데이터 노드 목록)를 HDFS 클라이언트에 전달합니다. 
3. HDFS 클라이언트는 그 목록을 가지고 데이터 블록을 검색해서 받습니다.

### **HDFS의 쓰기방식**
1. HDFS 클라이언트는 Name 노드에게 파일 쓰기를 요청합니다.
2. Name 노드는 Data 노드 목록을 HDFS 클라이언트에 전달합니다.
3. HDFS 클라이언트는 블록의 첫번째 복제본을 전달받은 Data 노드 중 하나에 write합니다.
4. 이후 Data 노드는 **복제 파이프라인**을 만들어 HDFS 클라이언트에 알립니다.
5. Data 노드는 Name 노드로 블록 보고서를 정기적으로 전송합니다.

## MapReduce  

하둡은 모든 연산을 MapReduce 방식으로 수행하는 것이 특징입니다. MapReduce는 여러 노드에 task를 분배하는 방법입니다. 

### **MapReduce 시스템의 구성요소** 

<center>
<img src="../assets/img/spark/mapreduce2.png" style="width:70%;">
</center>
<a href="https://www.opentutorials.org/course/2908/17055">출처:opentutorials</a>

|구성요소|부연설명|역할|
|---|---|---|
|MapReduce 클라이언트||데이터를 job의 형태로 jobtracker에 전달
|jobtracker|MapReduce의 Master 노드|전체 job을 스케줄링하고 모니터링함.
|tasktracker|MapReduce의 Slave 노드|사용자가 설정한 맵리듀스 프로그램을 실행하며, jobtracker로부터 작업을 요청받고, 맵 태스크와 리듀스 태스크를 생성합니다.|

### MapReduce의 사례


<center>
<b>"Welcome to Hadoop Class Hadoop is good Hadoop is bad" <br>이 문장에서 각 단어가 등장하는 빈도수를 세주세요</b>
</center>  

<img src="../assets/img/spark/mapreduce.png" style="width:70%;">

 **1단계 : Input Splits**  

데이터를 여러 노드에서 동시에 처리할 수 있게끔 split 합니다.  

**2단계 : Map** 

각 노드마다 쪼개진 데이터에 ``map``에 해당하는 일을 수행합니다. **Mapper는 Key/Value 형태로 데이터를 읽어서 다시 Key/Value 쌍 형태로 데이터를 출력해주는 작업을 합니다.** 하둡은 메타 데이터의 일부분을 노드에서 처리하게 함으로써 네트워크 트래픽을 방지할 수 있습니다.  

**3단계 : Shuffling**

map의 결과는 key값을 가지고 있습니다. 이제 공통의 key 값을 가진 결과끼리 리스트 형태로 조합되며, 이 리스트를 다시금 각 노드에 섞어서 분산시킵니다. 

**4단계 : Reduce**   

Reducer는 이렇게 key가 같은 데이터 리스트를 처리합니다. 


## YARN : 분산 컴퓨팅 환경 제공

``YARN``은 하둡을 구성하는 두가지 컴포넌트 중 하나로서, 하둡의 데이터 처리를 제어하고 조율하는 프로세스 스케줄러입니다. 스파크 응용 프로그램에서 가장 일반적으로 사용되는 프로세스 스케줄러이기도 합니다.  

하둡이 처음 나왔을 때는 ``HDFS``라는 파일 시스템과 전통적인 ``mapreduce``라는 컴퓨팅 시스템을 사용하였습니다. 하지만 이 방식에는 몇가지 문제가 있었습니다.  

### 기존 방식의 한계 및 극복

- mapreduce 말고도 spark와 같은 **다른 새로운 컴퓨팅 플랫폼**을 사용하고 싶다. 
- 여러개의 컴퓨팅 플랫폼을 실행하면 **리소스가 부족**하다.
- 하둡 클러스터의 서버에서 리소스 상태는 jobtracker에 의해 관리되었습니다. 이는 다른 컴퓨팅 클러스터와 연동하기 어려웠습니다.
- ``YARN``은 이 문제를 극복하기 위해 **리소스 관리 파트만 가져와서 다른 서비스와 연동 가능하게 구현한 시스템**. 

### YARN의 구성 요소  

<center>
<img src="../assets/img/spark/yarn.png" style="width:80%">
</center>
<p></p>

|구성요소|부연설명|역할|
|-------|---|---|
|애플리케이션 마스터 (AM) |소스 매니저가 노드 매니저를 실행하기 위해 할당한 첫 번째 컨테이너 | 노드 매니저에게 프로세스를 전달하는 응용프로그램|
|리소스 매니저|YARN 클러스터에서의 Master 서버| 클러스터 전체의 리소스를 관리|
|노드 매니저|YARN 클러스터의 Worker 서버|작업 시도 상태와 진행 상황을 애플리케이션 마스터에게 보고|
|*리소스|단위:컨테이너|CPU코어 + 메모리 |

YARN은 혼자서는 아무것도 못합니다. MapReduce나 Spark와 같은 분산 컴퓨팅 플랫폼이 있어야 기능을 합니다. 

**YARN의 작업 방식**

1. 클라이언트는 리소스 매니저에 응용프로그램을 제출합니다.
2. 애플리케이션 마스터 프로세스를 노드 매니저에 할당합니다.
3. 애플리케이션 마스터는 해당 애플리케이션을 위해 필요한 리소스를 계획합니다. 그리고 이러한 리소스를 리소스 매니저에게 요청합니다. 그럼 리소스 매니저는 리소스를 부여해서 애플리케이션 마스터 측에 전달합니다. 애플리케이션 마스터는 프로세스를 다른 노드 매니저에게 전달합니다. 
4. 각 노드 매니저는 작업 시도 상태, 진행 상황을 애플리케이션 마스터에게 전달합니다.
5. 애플리케이션 마스터는 응용 프로그램의 상태 및 진행률을 리소스 매니저에게 전달합니다.
6. 리소스 매니저는 응용 프로그램의 상태 및 진행률을 클라이언트에게 전달합니다.
  

더 자세한 내용은 <a href="https://www.popit.kr/what-is-hadoop-yarn/">이 곳</a>을 참고하면 좋을 것 같습니다.

### 요약

- 하둡: 그냥 hdfs와 일치하는 개념으로 주로 쓰인다. 스파크는 hdfs에서 파일을 읽고 hdfs로 파일을 쓴다. 
- YARN: 프로세스 스케줄러이자 리소스 스케줄러. 스파크의 응용 프로그램이라고 할 수 있다. YARN은 일반적으로 하둡의 hdfs와 함께 있다. 스파크의 처리를 병렬로 실행할 수 있도록 하둡 클러스터의 분산 노드 내에서 리소스를 관리해준다. 


---

### Reference
- <a href="#"> 파이썬을 활용한 스파크 프로그래밍 - 제프리 에이븐 </a>